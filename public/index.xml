<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Anything Data</title>
    <link>/</link>
    <description>Recent content on Anything Data</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 01 Jan 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Project 1</title>
      <link>/project/project1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/project/project1/</guid>
      <description>Predicting Churn with Telecom Data Exploring Numeric Data Exploring Categorical Data Modeling Model Test Performance Conclusion    Predicting Churn with Telecom Data 
This project used machine learning to predict customer churn in a telecom company. The data came from the UCI machine learning repository, and consisted of 19 columns, along with 3333 rows for training and 1667 for testing. First, I explored the data structure – the distributions, relationships between variables, and the class balances and imbalances.</description>
    </item>
    
    <item>
      <title>Project 2</title>
      <link>/project/project2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/project/project2/</guid>
      <description>Shiny App - Halo 5 Stats Purpose Technology Learned About the App    Shiny App - Halo 5 Stats 
Purpose The purpose of this project was to learn Shiny, R Studio’s powerful framework for packaging R code into web applications. With Shiny, the possibilities for communicating data are endless, including connecting to API’s or creating dashboards that update in real time. You might embed a machine learning model for producing results on the go, or track metrics that constantly update.</description>
    </item>
    
    <item>
      <title>Predicting Churn Using Tree Models</title>
      <link>/post/predicting-churn-using-tree-models/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/predicting-churn-using-tree-models/</guid>
      <description>Today I want to predict churn using data from a hypothetical telecom company. Although it isn’t real life data, it is based on real life data. The data are spread across 19 columns — 14 continuous, 4 categorical, and the outcome variable for prediction - “churn”. The dataset is small, with 3333 rows for training and 1667 for testing.
Before modeling, I need to explore the data structure – the distributions, class balances/imbalances, relationships between variables, etc.</description>
    </item>
    
    <item>
      <title>Linear Classification Models - Hepatic Dataset</title>
      <link>/post/linear-classification-models-hepatic-dataset/</link>
      <pubDate>Mon, 03 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/linear-classification-models-hepatic-dataset/</guid>
      <description>This post is following exercise 1 in Chapter 12 of Applied Predicative Modeling. Here I use the machine learning package CARET in R to make classification models; in particular, the linear classification models discussed in Chapter 12.
The dataset in question is about hepatic injury (liver damage). It includes a dataframe of biological related predictors of liver damage bio, a dataframe of chemical related predictors chem, and the response variable we are interested in, injury.</description>
    </item>
    
    <item>
      <title>First App - Halo 5 Stats</title>
      <link>/post/first-app-halo-5-stats/</link>
      <pubDate>Fri, 21 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/first-app-halo-5-stats/</guid>
      <description>“Halo 5 Stats” is my first ever Shiny app, which is themed around the the popular sci-fi shooter Halo 5. It combines beatiful data with Halo’s beautiful graphics - something that Halo fans and data enthusiasts undoubtedly love. You can use my app at https://jjohn9000.shinyapps.io/Halo_5_Stats/ After entering your gamertag, the app will automatically display your online game data using Microsoft’s Halo API and visualize your data with numerous customizable elements such as backdrop and plot colors.</description>
    </item>
    
    <item>
      <title>Part II: Chinese Classics&#39; Word/Network Plots</title>
      <link>/post/part-ii-chinese-classics-word-network-plots/</link>
      <pubDate>Fri, 08 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/part-ii-chinese-classics-word-network-plots/</guid>
      <description>This is a continuation in my series of exploratory text analysis of 3 Chinese classic works. In the previous post, I calculated word counts for each book, and visualized common words using bar charts. This time, I’d like to examine word use across the texts with network visualization. The goal is to help see what’s common and what’s different between the texts regarding word usage.
Network visualization is particularly helpful for discovering simularities and differences between objects - this is because nodes and edges can form connections and clusters (or stay isolated).</description>
    </item>
    
    <item>
      <title>Plotting Word Bigrams with 3 Chinese Classics</title>
      <link>/post/plotting-word-bigrams-with-3-chinese-classics/</link>
      <pubDate>Fri, 01 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/plotting-word-bigrams-with-3-chinese-classics/</guid>
      <description>In the last post, we saw frequencies of the most common words in the Analects, Zhuangzi, and Mozi texts. The faceted plot did an excellent job of capturing a generic “theme” of each text. However, I wondered how the results might change when plotting bigrams (2 word combinations of adjacent words) as opposed to single values.
This is where I ran into a problem with Tidytext – although it worked fine for tokenizing Chinese text into single character tokens, it did not perform as well at separating the text into bigrams.</description>
    </item>
    
    <item>
      <title>A Tidytext Analysis of 3 Chinese Classics</title>
      <link>/post/a-quasi-tidytext-analysis-of-3-chinese-classics/</link>
      <pubDate>Tue, 29 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/a-quasi-tidytext-analysis-of-3-chinese-classics/</guid>
      <description>For a long time I’ve admired the tidytext package and its wonderful companion book Text Mining with R. After reading it I thought, “Why not undertake a project of Chinese text analysis?” I am deeply interested in Chinese philosophy but I decided to keep the analysis narrow by selecting just three works - The Analects, Zhuangzi, and the Mozi.
Following similar pace with Tidytext, I first download my data. Here I use my package ctextclassics and specifically, the function get_books(c(.</description>
    </item>
    
    <item>
      <title>Ctextclassics, my First Package</title>
      <link>/post/ctextclassics-my-first-package/</link>
      <pubDate>Thu, 17 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/ctextclassics-my-first-package/</guid>
      <description>My latest update is a milestone! I have authored my first ever R package which is an API caller for ctext.org. Ctext hosts numerous pre-modern Chinese texts and my package makes them available to you. The scope is broad, but think philosophical works in Confucianism, Daoism, Legalism, military doctrines, history compilations, works in medicine, and many more.
The three main functions of ctextclassics are get_chapter(&amp;quot;book&amp;quot;, &amp;quot;chapter&amp;quot;) ,get_chapters(&amp;quot;book&amp;quot;, chapters), get_books(&amp;quot;book&amp;quot;) and the internal dataframe book_list which shows the available texts.</description>
    </item>
    
    <item>
      <title>Scraping for a Booklist of the Chinese Classics</title>
      <link>/post/scraping-for-a-booklist-of-the-chinese-classics/</link>
      <pubDate>Tue, 08 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/scraping-for-a-booklist-of-the-chinese-classics/</guid>
      <description>Last week I was considering a project that would be interesting and unique. I decided I would like to do a text analysis on classical Chinese texts, but wasn’t sure what kind of analysis regarding which texts. I decided to keep it small - and use five of the “core” Chinese classics - The Analects, The Mengzi, Dao De Jing, Zhuangzi, and Mozi. While there are many books in Confucianism, Daoism, and Moism, these texts are often used as the most representative examples of each “genre”.</description>
    </item>
    
    <item>
      <title>On Relocating to Github/Netlify</title>
      <link>/post/on-relocating-to-github-netlify/</link>
      <pubDate>Sun, 06 May 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/on-relocating-to-github-netlify/</guid>
      <description>Deep, labored breathing
Hello everyone, this is the opening post on my new blog, which I’m relocating from Wordpress to GitHub Pages and Netlify. It’s so nice I’ve given it a name - because nice things have names!
But, why was I panting? The relocation effort wasn’t easy. Why did I follow through with it? Becuase it is worth the effort. This post is evidence of my victory. Now please, let me explain.</description>
    </item>
    
    <item>
      <title>Visualizing European WW1 Defense Treaties with iGraph</title>
      <link>/post/visualizing-european-ww1-defense-treaties-with-igraph/</link>
      <pubDate>Sun, 01 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/visualizing-european-ww1-defense-treaties-with-igraph/</guid>
      <description>I suddenly got bit by a bug to learn about network analysis. So I recalled the Correlates of War Project having a dataset about alliances. I decided to revisit them and download the data for this new project, which you can do too.
To start small, I decided to visualize a certain topic, e.g., European Defense Treaties relating to World War I. For that purpose I filtered the dataset for treaties that occured between 1878 and 1914.</description>
    </item>
    
    <item>
      <title>Plotting Fortune 500 HQ&#39;s in R</title>
      <link>/post/plotting-fortune-500-hq-s-in-r/</link>
      <pubDate>Mon, 29 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/plotting-fortune-500-hq-s-in-r/</guid>
      <description>Today I’d like to work a little on geospatial mapping in R, so I’ve chosen a small dataset (only 256 kb) that can be plotted on a map. It the location information of Fortune 500 company headquarters in the US. You can download it from here.
R has several choices for plotting geospatial information. Here I use ggmap, however in the future I’ll check out the raster and sp packages. Anyway, let’s get started by loading in and cleaning the data.</description>
    </item>
    
    <item>
      <title>Top MBA Programs by US News</title>
      <link>/post/top-mba-programs-by-us-news/</link>
      <pubDate>Sat, 01 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/top-mba-programs-by-us-news/</guid>
      <description>Somebody once asked me for reccomendations on MBA programs based on rank and tuition. I didn’t have any information on hand, but knew how toget it. Webscraping.
Webscraping is an immensly useful tool for gathering data from webpages, when it isn’t hosted on an API or stored in a file somewhere. R’s best tool for webscraping is Rvest.
So I decided to scrape information on the US News website for university rankings, which has at least 20 pages of MBA probrams available.</description>
    </item>
    
  </channel>
</rss>